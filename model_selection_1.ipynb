{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Evaluation - Part 1\n",
    "\n",
    "When building supervised machine learning models, we need to solve two\n",
    "problems:\n",
    "\n",
    "1. **Model selection** - Finding the model that does as well as possible\n",
    "on our learning task.\n",
    "\n",
    "2. **Model evaluation** - Predicting **generalization error**, or the expected performance of our\n",
    "model on unseen data.\n",
    "\n",
    "Both are critical.  Without 1. we can't have an effective model and\n",
    "without 2. we can't *know* if we have an effective model.\n",
    "\n",
    "Once we have picked a particular machine learning algorithm, model\n",
    "selection comes down to the problem of **hyperparameter** tuning.\n",
    "Hyperparameters are parameters of our learning models that need to be\n",
    "selected before the model can be learned.  Examples include the\n",
    "maximum number of leaves in a decision tree or the number of hidden\n",
    "units in a neural network classifier.\n",
    "\n",
    "**WARNING:**  In the exercises below we will try several *BAD* approaches to model selection and evaluation.  These examples are not meant to illustrate the correct way of doing things, they are meant to show the consequences of doing things incorrectly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Naive Model Selection\n",
    "\n",
    "For now, let's focus entirely on model selection and disregard model\n",
    "evaluation.  The following cell will load a data set and use a\n",
    "decision tree regressor to fit a decision tree to the data. Try adjusting the `max_leaf_nodes` hyperparameter in order to minimize the MSE on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasource\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Grab our training data\n",
    "source = datasource.DataSource()\n",
    "X, y = source.gen_data(100, seed=100)\n",
    "\n",
    "# Build a decision tree regressor\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Evaluate the MSE of our decision tree on the training set\n",
    "y_predict = tree.predict(X)\n",
    "mse = np.sum((y - y_predict)**2) / y.size\n",
    "print(\"MSE: {:.4f}\".format(mse))\n",
    "\n",
    "# Plot the fit.\n",
    "plt.plot(X, y, '*')\n",
    "x_plt = np.linspace(0, 1, 400).reshape(400, 1)\n",
    "plt.plot(x_plt, tree.predict(x_plt))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* What value of the hyperparameter resulted in the lowest MSE?\n",
    "* Do you think that this MSE reflects how well this model will do on unseen data?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers: \n",
    "\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise above, you were able to tune the hyperparameters so as to *perfectly* fit the training data.  Now let's see what happens when we use this model on some new data drawn from the same underlying distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new, y_new = source.gen_data(1000, seed=200)\n",
    "y_new_predict = tree.predict(X_new)\n",
    "mse = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Using a Test Set for Hyperparameter Tuning and Evaluation\n",
    "\n",
    "In the exercise above, you were able to perfectly fit a training data set, but that didn't tell you anything about how well your model would perform on unseen data.  We might address this by splitting our limited data into a training set and a test set.  This is illustrated in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into a training and testing set...\n",
    "split_point = int(X.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_train = X[0:split_point, :]\n",
    "y_train = y[0:split_point]\n",
    "\n",
    "X_test = X[split_point::, :]\n",
    "y_test = y[split_point::]\n",
    "\n",
    "# Build a decision tree regressor using the TRAINING set\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the MSE of our decision tree on the TESTING set \n",
    "y_test_predict = tree.predict(X_test)\n",
    "mse = np.sum((y_test - y_test_predict)**2) / y_test.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* What hyperparameter settings gives us the lowest MSE on the testing data?  What is the MSE?  (I suggest writing a loop to systematically check all of the possible hyperparameter values. Bonus points for creating a plot with number of leaves vs. MSE.)\n",
    "* Do you think *this* MSE will be reflective of how well our model will perform on unseen data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this example we are using our test set for *both* model selection and model evaluation.  We used it for model selection by searching for a hyperparameter setting that minimizes error on the test set.  We use it for model evaluation by using our test set error as an estimate of the expected error rate on unobserved data.\n",
    "\n",
    "Let's see how our model does on some new, unobserved data drawn from the same distribution.  This cell will give us a good estimate of our *actual* generalization error.  (Note that in real-world problems we can't run a test like this because we don't have unlimited access to extra data that we can use to check our work.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=????) # Put your best hyperparameter here!\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Let's see how we do on unobserved data... \n",
    "X_new, y_new = source.gen_data(1000, seed=200)\n",
    "y_new_predict = tree.predict(X_new)\n",
    "mse = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Relative to Exercise 1, where we just looked for the model that best fit our training data, would you say that our train/test split was beneficial in terms of model selection, i.e. did we end up with a better model? Justify your answer.\n",
    "* Would you say that our train/test split was beneficial in terms of model evaluation, i.e. were we able make a better prediction of our generalization error?  Justify your answer.\n",
    "* Do you see any problems here in terms of model selection or evaluation?  How accurate was our prediction of generalization error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click [here](model_selection_2.ipynb) to open the next page of exercises..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
