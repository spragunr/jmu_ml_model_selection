{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Evaluation - Part 1\n",
    "\n",
    "When building supervised machine learning models, we need to solve two\n",
    "problems:\n",
    "\n",
    "1. **Model selection** - Finding the model that does as well as possible\n",
    "on our learning task.\n",
    "\n",
    "2. **Model evaluation** - Predicting **generalization error** , or the expected performance, of our\n",
    "model on unseen data.\n",
    "\n",
    "Both are critical.  Without 1. we can't have an effective model and\n",
    "without 2. we can't *know* if we have an effective model.\n",
    "\n",
    "Once we have picked a particular machine learning algorithm, model\n",
    "selection comes down to the problem of **hyperparameter** tuning.\n",
    "Hyperparameters are parameters of our learning models that need to be\n",
    "selected before the model can be learned.  Examples include the\n",
    "maximum number of leaves in a decision tree or the number of hidden\n",
    "units in a neural network classifier.\n",
    "\n",
    "**WARNING:**  In the exercises below we will try several *BAD* approaches to model selection and evaluation.  These examples are not meant to illustrate the correct way of doing things, they are meant to show the consequences of doing things incorrectly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 - Naive Model Selection\n",
    "\n",
    "For now, let's focus entirely on model selection and disregard model\n",
    "evaluation.  The following cell will load a data set and use a\n",
    "decision tree regressor to fit a decision tree to the data. Try adjusting the `max_leaf_nodes` hyperparameter in order to minimize the MSE on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2112\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasource\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Grab our training data\n",
    "source = datasource.DataSource()\n",
    "X, y = source.gen_data(100, seed=100)\n",
    "\n",
    "# Build a decision tree regressor\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "tree.fit(X, y)\n",
    "\n",
    "# Evaluate the MSE of our decision tree on the training set\n",
    "y_predict = tree.predict(X)\n",
    "mse = np.sum((y - y_predict)**2) / y.size\n",
    "print(\"MSE: {:.4f}\".format(mse))\n",
    "\n",
    "# Plot the fit.\n",
    "plt.plot(X, y, '*')\n",
    "x_plt = np.linspace(0, 1, 400).reshape(400, 1)\n",
    "plt.plot(x_plt, tree.predict(x_plt))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* What value of the hyperparameter resulted in the lowest MSE?\n",
    "* Do you think that this MSE reflects how well this model will do on unseen data?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers: \n",
    "\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise above, you were able to tune the hyperparameters to so as to *perfectly* fit the training data.  Now let's see what happens when we use this model on some new data drawn from the same underlying distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.2316\n"
     ]
    }
   ],
   "source": [
    "X_new, y_new = source.gen_data(1000, seed=200)\n",
    "y_new_predict = tree.predict(X_new)\n",
    "mse = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 - Using a Test Set for Hyperparameter Tuning and Evaluation\n",
    "\n",
    "In the exercise above, you were able to perfectly fit a training data set, but that didn't tell you anything about how well your model would perform on unseen data.  We might address this by splitting our limited data into a training set and a test set..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.0966\n"
     ]
    }
   ],
   "source": [
    "# Split our data into a training and testing set...\n",
    "split_point = int(X.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_train = X[0:split_point, :]\n",
    "y_train = y[0:split_point]\n",
    "\n",
    "X_test = X[split_point::, :]\n",
    "y_test = y[split_point::]\n",
    "\n",
    "# Build a decision tree regressor using the TRAINING set\n",
    "tree = DecisionTreeRegressor(max_leaf_nodes=3)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the MSE of our decision tree on the TESTING set \n",
    "y_test_predict = tree.predict(X_test)\n",
    "mse = np.sum((y_test - y_test_predict)**2) / y_test.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "* What hyperparameter settings gives us the lowest MSE on the testing data?  What is the MSE?  (I suggest writing a loop to systematically check all of the possible hyperparameter values. Bonus points for creating a plot with number of leaves vs. MSE.)\n",
    "* Do you think *this* MSE will be reflective of how well our model will perform on unseen data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our model does on some new, unobserved data drawn from the same distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-c38f65772680>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-c38f65772680>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    tree = DecisionTreeRegressor(max_leaf_nodes=????) # Put your best hyperparameter here!\u001b[0m\n\u001b[0m                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=????) # Put your best hyperparameter here!\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Let's see how we do on unobserved data... \n",
    "X_new, y_new = source.gen_data(1000, seed=200)\n",
    "y_new_predict = tree.predict(X_new)\n",
    "mse = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "* Would you say that our train/test split was beneficial in terms of model selection? Why or why not?\n",
    "* Would you say that our train/test split was beneficial in terms of model evaluation? Why or why not?\n",
    "* Do you see any problems here in terms of model selection or evaluation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers:\n",
    "\n",
    "* \n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Click [here](model_selection_2.ipynb) to open the next page of exercises..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
