{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Evaluation - Part 2\n",
    "\n",
    "## Activity 3 - Cross Validation\n",
    "\n",
    "In the previous activity, we made the classic mistake of using our validation data to predict our generalization error. This tends to give misleadingly optimistic predictions about how well we will do on unobserved data. Remember that we carefully picked our hyperparameter values to do as well as possible *on our held-out data*. We shouldn't be surprised when our model performs better on that data than on unobserved data. This problem is particularly acute if our data set is small.\n",
    "\n",
    "The traditional solution is to divide our data into three disjoint sets: **training**, **validation**, and **testing**:\n",
    "* The **training** set is used to fit the model.\n",
    "* The **validation** set is used to evaluate models for the purpose of hyperparameter selection. \n",
    "* The **test** set is kept in a locked room guarded by jaguars. We only look at the testing set ONCE, when we have finalized our model. That way our performance on the test set gives us an unbiased estimate of our generalization error.\n",
    "\n",
    "This traditional approach is fine if we have a lot of data to work with. If the data set is small, we are faced with a painful dilemma: More validation data means better model selection. More testing data means more accurate model evaluation. More training data means better models. Any data we use for one purpose can't be used for the others.\n",
    "\n",
    "**Cross validation** is one way to use limited data more effectively.  The cells below walk us through an example of using cross validation for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reimport and reload everything...\n",
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datasource\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Grab our training data\n",
    "source = datasource.DataSource()\n",
    "X, y = source.gen_data(100, seed=100)\n",
    "\n",
    "# Split our data into a training and testing set...\n",
    "split_point = int(X.shape[0] * .8) # Use 80% of the data to train the model\n",
    "\n",
    "X_train = X[0:split_point, :]\n",
    "y_train = y[0:split_point]\n",
    "\n",
    "X_test = X[split_point::, :] # This data will ONLY be used for final evaluation.\n",
    "y_test = y[split_point::]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows how we can use the scikit-learn `KFold` class to automatically split up our training data for k-fold cross validation. Take a minute to read through this code to make sure you understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "folds = 10\n",
    "max_max_leaves = 80\n",
    "kf = KFold(n_splits=folds)\n",
    "\n",
    "mses = np.zeros((folds, max_max_leaves - 2)) # (can't have 0 or 1 leaves)\n",
    "\n",
    "# Loop over all of the hyperparameter settings\n",
    "for max_leaves in range(2, max_max_leaves):\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    # Evaluate each one K-times\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X[train_index], X[val_index]\n",
    "        y_tr, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        tree = DecisionTreeRegressor(max_leaf_nodes=max_leaves)\n",
    "        tree.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_val_predict = tree.predict(X_val)\n",
    "        mses[k, max_leaves - 2] = np.sum((y_val - y_val_predict)**2) / y_val.size\n",
    "        \n",
    "        k += 1\n",
    "        \n",
    "# Average across the k folds\n",
    "mse_avg = np.mean(mses, axis=0)\n",
    "\n",
    "plt.plot(np.arange(2, max_max_leaves), mse_avg)\n",
    "plt.xlabel('max leaves')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are real experts in scikit-learn, we can automate some of this by using the `cross_val_score` function:  \n",
    "\n",
    "(There are also library routines for [automating the entire process of hyperparameter tuning](https://scikit-learn.org/stable/modules/grid_search.html).) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "mses = np.zeros((folds,max_max_leaves - 2))\n",
    "\n",
    "# Loop over all of the hyperparameter settings\n",
    "for size in range(2, max_max_leaves):\n",
    "    tree = DecisionTreeRegressor(max_leaf_nodes=size)\n",
    "    \n",
    "    # Returns an array of cross validation results.\n",
    "    mses[:, size - 2] = -cross_val_score(tree, X_train, y_train, \n",
    "                                         cv=folds, scoring='neg_mean_squared_error')\n",
    "    \n",
    "mse_avg = np.mean(mses, axis=0)\n",
    "\n",
    "plt.plot(np.arange(2, 80), mse_avg)\n",
    "plt.show()\n",
    "plt.xlabel('max leaves')\n",
    "plt.ylabel('MSE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "* Based on the results above, what is the most promising hyperparameter value?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a value for our hyperparameter, let's train our final model on the *full* training set and use our locked-away testing set to predict model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(max_leaf_nodes=????) # Put your best hyperparameter here!\n",
    "\n",
    "# Train using ALL the training data\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Test on held-out testing data\n",
    "y_test_predict = tree.predict(X_test)\n",
    "mse = np.sum((y_test - y_test_predict)**2) / y_test.size\n",
    "\n",
    "print(\"Predicted MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since none of the data we are testing on here was used *in any way* to design or fit the model, this value should give us an unbiased estimate of our generalization error.  Let's try testing on some new unobserved data to see how good our estimate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's see how we do on unobserved data... \n",
    "X_new, y_new = source.gen_data(5000, seed=200)\n",
    "y_new_predict = tree.predict(X_new)\n",
    "mse = np.sum((y_new - y_new_predict)**2) / y_new.size\n",
    "print(\"MSE: {:.4f}\".format(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "Take a look back at the results from Exercise 2 and answer the following questions:\n",
    "\n",
    "* Did the cross validation approach improve our results in terms of model selection?  Justify your answer.\n",
    "* Did maintaining a proper test set improve our results in terms of model evaluation?  Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "* \n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 - Bias Variance Decomposition\n",
    "\n",
    "Earlier in the semester we spent some time considering the bias/variance decomposition: \n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = (Bias[\\hat{f}(x)])^2 + Variance[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "### Questions\n",
    "\n",
    "* What term above corresponds to the notion of *generalization error* that we have been discussing?\n",
    "* How is hyperparameter selection related to the Bias term?\n",
    "* How is the training set size related to the Variance term?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers\n",
    "\n",
    "*\n",
    "*\n",
    "*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 - Nested Cross Validation\n",
    "\n",
    "There is still one drawback of our approach from the previous exercise:  Our testing set was quite small, which limits the reliability of our model evaluation. Maybe the test set happens to contain particularly difficult or particularly easy instances? \n",
    "\n",
    "In a perfect world, we would like to be able to use *all* of our data for training and validation and *all* of our data for testing.  In a way, we can accomplish this by using **nested cross validation**.  This involves generating K-fold data splits at two levels. At the top level, we iterate over Testing/Training splits for the purpose of model evaluation.  For *each* Training/Testing split we iterate over Training/Validation splits for the purpose of model tuning.  This page provides an [example of nested cross-validation using scikit-learn](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html). \n",
    "\n",
    "If you have time, go ahead and use nested cross validation to redo our example above.  Does it improve our estimate of generalization error? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
